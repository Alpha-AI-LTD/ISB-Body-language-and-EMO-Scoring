{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prior to starting the application, you need to obtain your free Groq API key from https://console.groq.com/keys. ⛔\n",
        "\n",
        "**Replace the key at this point in the code below ⬇️**\n",
        "\n",
        "GROQ_API_KEY = \"abcdefghijkl_1234567890\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "qQ7eeMqdb94m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY_USER = \"gsk_djt1FIWJ6TjVrSamnbkOWGdyb3FYIPajsahfjshlfd\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "UUy3vraOcKK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the value for scoring criteria. Set \"True\" for whichever component you want to use and leave it \"False\" for the rest."
      ],
      "metadata": {
        "id": "SxgVok2KBanb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visibility Settings — set exactly one to True\n",
        "only_hands_visible = False\n",
        "both_hands_and_legs_visible = False\n",
        "no_hands_no_legs_visible = True"
      ],
      "metadata": {
        "id": "yQ2kST20AsCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3KozkPC7eucI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run the application Go to Runtime button in the Navigation bar above and click on the Run All option. The Code will now run automatically.\n",
        "\n",
        "# Sometimes, there could be an error that occurs when the Stage 2 code is running. Do not worry. Simply go to Runtime > Restart Session > Run All. It will start again but this time the error will not be there. It occurs because of some installation defaults in the code (No need to bother).\n",
        "\n",
        "**NOTE - Remember you have added the API key above prior to doing this step.**\n",
        "\n",
        "**For any concerns write to info@alphaai.biz**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4y-vPB2ncVtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lcd4_e6PexL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 - The dependencies will install automatically. Do not terminate the session, close the browser tab or interrrupt the execution by any means possible."
      ],
      "metadata": {
        "id": "vx0klwWUaktx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWZM1OwCNfkV"
      },
      "outputs": [],
      "source": [
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas mediapipe librosa speechrecognition opencv-python ffmpeg-python langchain-groq\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install numpy==1.26.4\n",
        "!pip install mediapipe==0.10.14\n",
        "!pip install pandas==2.2.2\n",
        "!pip install librosa==0.10.2\n",
        "!pip install speechrecognition==3.10.4\n",
        "!pip install opencv-python==4.10.0.84\n",
        "!pip install ffmpeg-python==0.2.0\n",
        "!pip install langchain-groq==0.3.0\n",
        "\n",
        "# Install Whisper for speech recognition\n",
        "!pip install openai-whisper==20231117"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If the above installation is successful then there would be a number inclosed within the square bracket. For example [1] or [2]."
      ],
      "metadata": {
        "id": "Q7O2cRala7hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yqSf_PmCez3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 - Here the code will run automatically and ask you to upload your video file for analysis."
      ],
      "metadata": {
        "id": "txhvglLDbRB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import groq\n",
        "import httpx\n",
        "print(f\"groq: {groq.__version__}, httpx: {httpx.__version__}\")"
      ],
      "metadata": {
        "id": "3gzdvxMmw9zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key=GROQ_API_KEY_USER)\n",
        "print(\"Groq client initialized!\")"
      ],
      "metadata": {
        "id": "Qs7dvmxbxAp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE - In case you do not wish to work with ranges then refer to the code below. Nothing that technical just look for the System Prompt and Report Generation and refer to the Comment/Un-Comment instructions there.**"
      ],
      "metadata": {
        "id": "PU5cu4uInfAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import files\n",
        "\n",
        "# Initialize MediaPipe Holistic\n",
        "try:\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    holistic = mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize MediaPipe: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq client\n",
        "GROQ_API_KEY = GROQ_API_KEY_USER\n",
        "try:\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq client: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq LLM for report generation\n",
        "try:\n",
        "    llm = ChatGroq(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=2000,\n",
        "        api_key=GROQ_API_KEY\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq LLM: {e}\")\n",
        "    exit(1)\n",
        "### UNCOMMENT THE BELOW PROMPT IF YOU WANT SINGULAR VALUES INSTEAD OF RANGES ###\n",
        "\n",
        "# # System prompt for three-flag logic\n",
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"\"\"\n",
        "# You are an expert in evaluating public speaking skills for management students, using a scoring system aligned with uSpeek's criteria (all scores out of 5). Given data on body language, facial expressions, speech transcript, audio characteristics, and component scores, generate a detailed report. The report must:\n",
        "\n",
        "# - **Body Language**: Evaluate posture, gestures, engagement according to flags:\n",
        "#   - `no_hands_no_legs_visible=True`: compute dynamic baseline by applying posture+gesture formula but skip visibility penalties.\n",
        "#   - `only_hands_visible=True`: score purely on gesture frequency (0–5) minus engagement penalty (0.5 if freq < 0.5 else 0.2).\n",
        "#   - `both_hands_and_legs_visible=True`: apply full formula: baseline 3.5 minus posture penalty plus gesture bonus minus engagement penalty.\n",
        "#   Target ~{body_language_target}/5.\n",
        "\n",
        "# - **Facial Expressions**: Score 2.0/5 for smiling ratio <10%, 3.0/5 for ≥10%.\n",
        "\n",
        "# - **Speech Quality**: Analyze modulation (cap at 4.0), pitch (~300 Hz) using YIN, volume (~60 dB). Penalize deviations: pitch penalty abs(avg_pitch-300)/500, volume penalty abs(avg_volume-60)/30. Target ~{speech_quality_target}/5.\n",
        "\n",
        "# - **Content Quality**: Evaluate clarity, relevance, impact. Penalize filler words (ratio >5%, reduce by 0.15/5). Target ~{content_quality_target}/5.\n",
        "\n",
        "# - **Final Score**: Average component scores, targeting ~{final_score_target}/5. Scale: 1 = Many areas to improve, …, 5 = Super Star.\n",
        "\n",
        "# - **Recommendations**: Suggest improvements for each component.\n",
        "\n",
        "# Use a professional tone and clear structure.\n",
        "# \"\"\"),\n",
        "#     (\"human\", \"\"\"\n",
        "# Body Language: {body_language}\n",
        "# Facial Expressions: {facial_expressions}\n",
        "# Speech Transcript: {transcript}\n",
        "# Audio Characteristics: Pitch std: {pitch_std}, Volume std: {volume_std}, Avg pitch: {avg_pitch} Hz, Avg volume: {avg_volume} dB\n",
        "# Filler Words: {filler_words}\n",
        "# Pet Words: {pet_words}\n",
        "# Preliminary Scores:\n",
        "# - Body Language: {body_language_score}/5\n",
        "# - Facial Expressions: {facial_expressions_score}/5\n",
        "# - Speech Quality: {speech_quality_score}/5\n",
        "# - Content Quality: {content_quality_score}/5\n",
        "# \"\"\"),\n",
        "# ])\n",
        "\n",
        "### COMMENT THE BELOW PROMPT IF YOU WANT TO USE SINGLE VALUES FOR THE SCORE INSTEAD OF RANGES ###\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "You are an expert in evaluating public speaking skills for management students, using a scoring system aligned with uSpeek's criteria (all scores out of 5). Given data on body language, facial expressions, speech transcript, audio characteristics, and component scores, generate a detailed report. The report must:\n",
        "\n",
        "- **Body Language**: Evaluate posture, gestures, engagement according to flags:\n",
        "  - `no_hands_no_legs_visible=True`: compute dynamic baseline by applying posture+gesture formula but skip visibility penalties.\n",
        "  - `only_hands_visible=True`: score purely on gesture frequency (0–5) minus engagement penalty (0.5 if freq < 0.5 else 0.2).\n",
        "  - `both_hands_and_legs_visible=True`: apply full formula: baseline 3.5 minus posture penalty plus gesture bonus minus engagement penalty.\n",
        "  Target ~{body_language_target}/5.\n",
        "\n",
        "- **Facial Expressions**: Score 2.0/5 for smiling ratio <10%, 3.0/5 for ≥10%.\n",
        "\n",
        "- **Speech Quality**: Analyze modulation (cap at 4.0), pitch (~300 Hz) using YIN, volume (~60 dB). Penalize deviations: pitch penalty abs(avg_pitch-300)/500, volume penalty abs(avg_volume-60)/30. Target ~{speech_quality_target}/5.\n",
        "\n",
        "- **Content Quality**: Evaluate clarity, relevance, impact. Penalize filler words (ratio >5%, reduce by 0.15/5). Target ~{content_quality_target}/5.\n",
        "\n",
        "- **Final Score**: Compute a single score by averaging components, then present a range of +/- 0.2 around that score (clamped to [1,5]).\n",
        "\n",
        "- **Recommendations**: Suggest improvements for each component.\n",
        "\n",
        "Use a professional tone and clear structure.\n",
        "\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "Body Language: {body_language}\n",
        "Facial Expressions: {facial_expressions}\n",
        "Speech Transcript: {transcript}\n",
        "Audio Characteristics: Pitch std: {pitch_std}, Volume std: {volume_std}, Avg pitch: {avg_pitch} Hz, Avg volume: {avg_volume} dB\n",
        "Filler Words: {filler_words}\n",
        "Pet Words: {pet_words}\n",
        "Preliminary Scores:\n",
        "- Body Language: {body_language_score}/5\n",
        "- Facial Expressions: {facial_expressions_score}/5\n",
        "- Speech Quality: {speech_quality_score}/5\n",
        "- Content Quality: {content_quality_score}/5\n",
        "\"\"\"),\n",
        "])\n",
        "\n",
        "# Audio extraction\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    stream = ffmpeg.input(video_path)\n",
        "    stream = ffmpeg.output(stream, audio_path, acodec='pcm_s16le', ar=16000, ac=1)\n",
        "    ffmpeg.run(stream, overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
        "\n",
        "# Transcription\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        with open(audio_path, 'rb') as f:\n",
        "            res = client.audio.transcriptions.create(\n",
        "                file=(os.path.basename(audio_path), f.read()),\n",
        "                model='whisper-large-v3', response_format='json', language='en', temperature=0.0\n",
        "            )\n",
        "        return res.text\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# Filler & pet words\n",
        "\n",
        "def analyze_filler_pet_words(transcript):\n",
        "    words = transcript.lower().split()\n",
        "    fillers = ['and','that','really','now','just','um','uh','like']\n",
        "    pets = ['i','to','the','of']\n",
        "    fcnt = {w:words.count(w) for w in fillers if words.count(w)>0}\n",
        "    pcnt = {w:words.count(w) for w in pets if words.count(w)>0}\n",
        "    total = len(words)\n",
        "    fr = sum(fcnt.values())/total if total else 0\n",
        "    return fcnt, pcnt, fr\n",
        "\n",
        "# Audio analysis\n",
        "\n",
        "def analyze_audio(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    f0 = librosa.yin(y, fmin=80, fmax=500, sr=sr)\n",
        "    avg_pitch = float(np.median(f0[f0>0])) if np.any(f0>0) else 300.0\n",
        "    pitch_std = float(np.std(f0[f0>0])) if np.any(f0>0) else 0.0\n",
        "    rms = librosa.feature.rms(y=y)[0]\n",
        "    avg_volume = float(20*np.log10(np.mean(rms)+1e-10)+60)\n",
        "    volume_std = float(np.std(rms))\n",
        "    mod_score = min(4.0,(pitch_std/100 + volume_std/0.01)*0.8)\n",
        "    pitch_pen = abs(avg_pitch-300)/500\n",
        "    vol_pen = abs(avg_volume-60)/30\n",
        "    speech_score = min(5,max(1,mod_score - pitch_pen - vol_pen))\n",
        "    return pitch_std, volume_std, avg_pitch, avg_volume, speech_score\n",
        "\n",
        "# Content analysis\n",
        "\n",
        "def analyze_content(transcript, fr):\n",
        "    p = ChatPromptTemplate.from_messages([\n",
        "        ('system','Evaluate clarity, relevance, impact. Penalize filler >5% by -0.15.'),\n",
        "        ('human','{transcript}')\n",
        "    ])\n",
        "    resp = (p | llm).invoke({'transcript':transcript,'filler_ratio':fr})\n",
        "    try:\n",
        "        base = float(resp.content.strip())\n",
        "    except:\n",
        "        base = 3.5\n",
        "    penalty = 0.15 if fr>0.05 else 0\n",
        "    return min(5,max(1,base-penalty))\n",
        "\n",
        "# Video analysis\n",
        "\n",
        "def analyze_video(video_path, only_hands_visible=False, both_hands_and_legs_visible=False, no_hands_no_legs_visible=False):\n",
        "    if [only_hands_visible, both_hands_and_legs_visible, no_hands_no_legs_visible].count(True)!=1:\n",
        "        raise ValueError('Set exactly one visibility flag to True')\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): raise ValueError('Cannot open video')\n",
        "    cnt=gest=0; post_list=[]; expr=[]\n",
        "    while True:\n",
        "        ret, frm = cap.read()\n",
        "        if not ret: break\n",
        "        rgb=cv2.cvtColor(frm,cv2.COLOR_BGR2RGB)\n",
        "        res=holistic.process(rgb)\n",
        "        if res.pose_landmarks:\n",
        "            ls=res.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
        "            rs=res.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
        "            post_list.append(abs(ls.y-rs.y))\n",
        "            if res.left_hand_landmarks or res.right_hand_landmarks: gest+=1\n",
        "        if res.face_landmarks:\n",
        "            m1=res.face_landmarks.landmark[61]; m2=res.face_landmarks.landmark[291]\n",
        "            expr.append('Smiling' if np.hypot(m2.x-m1.x,m2.y-m1.y)>0.05 else 'Neutral')\n",
        "        cnt+=1\n",
        "    cap.release()\n",
        "    gf=gest/cnt if cnt else 0\n",
        "    eng_pen=0.5 if gf<0.5 else 0.2\n",
        "    avg_post=np.mean(post_list) if post_list else 0\n",
        "    post_pen=(avg_post/0.1)*0.3\n",
        "    if no_hands_no_legs_visible:\n",
        "        raw=3.5-post_pen+gf*2-eng_pen\n",
        "        desc=f'Dynamic baseline: posture avg={avg_post:.3f}, gesture freq={gf:.3f}'\n",
        "    elif only_hands_visible:\n",
        "        raw=gf*5-eng_pen; desc=f'Gestures only (freq={gf:.3f})'\n",
        "    else:\n",
        "        raw=3.5-post_pen+gf*2-eng_pen; desc=f'Full-body: posture avg={avg_post:.3f}, gesture freq={gf:.3f}'\n",
        "    body_sc=round(min(5,max(1,raw)),1)\n",
        "    if expr:\n",
        "        sr = expr.count('Smiling')/len(expr)\n",
        "        fe_sc=2.0 if sr<0.1 else 3.0; fe_desc=f'Smiling {sr*100:.1f}%'\n",
        "    else:\n",
        "        fe_sc=2.0; fe_desc='No facial data'\n",
        "    return desc, fe_desc, body_sc, fe_sc\n",
        "\n",
        "# Report generation\n",
        "\n",
        "def generate_report(video_path,output_path,only_hands_visible=False,both_hands_and_legs_visible=False,no_hands_no_legs_visible=False):\n",
        "    ap='temp.wav'\n",
        "    try:\n",
        "        extract_audio(video_path,ap)\n",
        "        tr=transcribe_audio(ap)\n",
        "        ps,vs,ap_pitch,av,ss=analyze_audio(ap)\n",
        "        fcnt,pcnt,fr=analyze_filler_pet_words(tr)\n",
        "        cs=analyze_content(tr,fr)\n",
        "        bl_desc,fe_desc,bl_sc,fe_sc=analyze_video(video_path,only_hands_visible,both_hands_and_legs_visible,no_hands_no_legs_visible)\n",
        "        final=np.mean([bl_sc,fe_sc,ss,cs])\n",
        "        # Targets\n",
        "        blt=2.5 if ps<800 else 3.7; sqt=3.0 if ps<800 else 3.5; cqt=3.2 if fr>0.05 else 3.5; fst=2.9 if ps<800 else 3.4\n",
        "        avg_score=np.mean([bl_sc,fe_sc,ss,cs])\n",
        "        low  = round(max(1, avg_score - 0.2), 1)\n",
        "        high = round(min(5, avg_score + 0.2), 1)\n",
        "\n",
        "        resp=(prompt|llm).invoke({\n",
        "            'body_language':bl_desc,\n",
        "            'facial_expressions':fe_desc,\n",
        "            'transcript':tr,\n",
        "            'pitch_std':ps,\n",
        "            'volume_std':vs,\n",
        "            'avg_pitch':ap_pitch,\n",
        "            'avg_volume':av,\n",
        "            'filler_words':str(fcnt),\n",
        "            'pet_words':str(pcnt),\n",
        "            'body_language_score':bl_sc,\n",
        "            'facial_expressions_score':fe_sc,\n",
        "            'speech_quality_score':ss,\n",
        "            'content_quality_score':cs,\n",
        "            'body_language_target':blt,\n",
        "            'speech_quality_target':sqt,\n",
        "            'content_quality_target':cqt,\n",
        "            'final_score_target':fst\n",
        "        })\n",
        "        ### UNCOMMENT THE BELOW LINE TO ENABLE SINGLE VALUE SCORES ###\n",
        "        # report=f\"{resp.content}\\n\\n**Final Score**: {round(final,1)}/5\"\n",
        "\n",
        "        ### COMMENT THE BELOW LINE TO ENABLE SINGLE VALUE SCORES ###\n",
        "        report=f\"{resp.content}\\n\\n**Final Score Range**: {low}-{high}/5\"\n",
        "        with open(output_path,'w') as f: f.write(report)\n",
        "        return report\n",
        "    finally:\n",
        "        if os.path.exists(ap): os.remove(ap)\n",
        "\n",
        "# Main\n",
        "\n",
        "def main():\n",
        "    print('Upload your video')\n",
        "    u=files.upload()\n",
        "    if not u: print('No file'); return\n",
        "    vp=list(u.keys())[0]; op='evaluation_report.txt'\n",
        "    rep=generate_report(vp,op,only_hands_visible,both_hands_and_legs_visible,no_hands_no_legs_visible)\n",
        "    print(rep)\n",
        "    files.download(op)\n",
        "\n",
        "if __name__=='__main__': main()"
      ],
      "metadata": {
        "id": "P4qnGpO4NvAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank for using our free tool! If it is posssilbe for you then do share it along and let others benefit from the same 🤗\n",
        "\n",
        "**Credits: Alpha AI Team (www.alphaai.biz)**"
      ],
      "metadata": {
        "id": "aK6Qkb4le27L"
      }
    }
  ]
}