{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prior to starting the application, you need to obtain your free Groq API key from https://console.groq.com/keys. ‚õî\n",
        "\n",
        "**Replace the key at this point in the code below ‚¨áÔ∏è**\n",
        "\n",
        "GROQ_API_KEY = \"abcdefghijkl_1234567890\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "qQ7eeMqdb94m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY_USER = \"gsk_djt1FIWJ6TjVrSamnbkOWGdyasfhljsahdflkhsalhalskh\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "UUy3vraOcKK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3KozkPC7eucI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run the application Go to Runtime button in the Navigation bar above and click on the Run All option. The Code will now run automatically.\n",
        "\n",
        "**NOTE - Remember you have added the API key above prior to doing this step.**\n",
        "\n",
        "**For any concerns write to info@alphaai.biz**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4y-vPB2ncVtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lcd4_e6PexL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 - The dependencies will install automatically. Do not terminate the session, close the browser tab or interrrupt the execution by any means possible."
      ],
      "metadata": {
        "id": "vx0klwWUaktx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWZM1OwCNfkV"
      },
      "outputs": [],
      "source": [
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas mediapipe librosa speechrecognition opencv-python ffmpeg-python langchain-groq\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install numpy==1.26.4\n",
        "!pip install mediapipe==0.10.14\n",
        "!pip install pandas==2.2.2\n",
        "!pip install librosa==0.10.2\n",
        "!pip install speechrecognition==3.10.4\n",
        "!pip install opencv-python==4.10.0.84\n",
        "!pip install ffmpeg-python==0.2.0\n",
        "!pip install langchain-groq==0.3.0\n",
        "\n",
        "# Install Whisper for speech recognition\n",
        "!pip install openai-whisper==20231117"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If the above installation is successful then there would be a number inclosed within the square bracket. For example [1] or [2]."
      ],
      "metadata": {
        "id": "Q7O2cRala7hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yqSf_PmCez3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 - Here the code will run automatically and ask you to upload your video file for analysis."
      ],
      "metadata": {
        "id": "txhvglLDbRB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import groq\n",
        "import httpx\n",
        "print(f\"groq: {groq.__version__}, httpx: {httpx.__version__}\")"
      ],
      "metadata": {
        "id": "3gzdvxMmw9zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key=GROQ_API_KEY_USER)\n",
        "print(\"Groq client initialized!\")"
      ],
      "metadata": {
        "id": "Qs7dvmxbxAp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# Initialize MediaPipe Holistic\n",
        "try:\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    holistic = mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize MediaPipe: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq client\n",
        "GROQ_API_KEY = GROQ_API_KEY_USER\n",
        "try:\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq client: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq LLM for report generation\n",
        "try:\n",
        "    llm = ChatGroq(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=2000,\n",
        "        api_key=GROQ_API_KEY\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq LLM: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Prompt for uSpeek's criteria\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert in evaluating public speaking skills for management students, using a scoring system aligned with uSpeek's criteria (all scores out of 5). Given data on body language, facial expressions, speech transcript, audio characteristics, and preliminary scores, generate a detailed report. The report must:\n",
        "    - **Body Language**: Evaluate posture, gestures, engagement. If upper_torso=True, focus on upper body without leg penalty. If upper_torso=False, penalize missing legs (0.5/5). Penalize missing hands (0.5/5) and low engagement (0.5/5 if gesture frequency <0.5, else 0.2/5). Reward frequent gestures and balanced posture. Target ~{body_language_target}/5.\n",
        "    - **Facial Expressions**: Score 2.0/5 for smiling ratio <10%, 3.0/5 for ‚â•10%. Target ~2.0/5 for limited expressiveness.\n",
        "    - **Speech Quality**: Analyze modulation, pitch (~300 Hz), volume (~60 dB). Cap at 4.0/5 for modulation. Penalize pitch/volume deviations. Target ~{speech_quality_target}/5.\n",
        "    - **Content Quality**: Evaluate clarity, relevance, impact. Penalize filler words (ratio >5%, reduce by 0.3/5). Target ~{content_quality_target}/5.\n",
        "    - **Final Score**: Average component scores, targeting ~{final_score_target}/5. Scale: 1 = Many areas to improve, 2 = Improve your show, 3 = Good show, 4 = Fabulous show, 5 = Super Star.\n",
        "    - **Recommendations**: Suggest improvements (e.g., reduce fillers, increase gestures).\n",
        "    Use a professional tone, structure clearly, and prioritize engagement and clarity.\"\"\"),\n",
        "    (\"human\", \"\"\"Body Language: {body_language}\n",
        "Facial Expressions: {facial_expressions}\n",
        "Speech Transcript: {transcript}\n",
        "Audio Characteristics: Pitch variation (std): {pitch_std}, Volume variation (std): {volume_std}, Average pitch: {avg_pitch} Hz, Average volume: {avg_volume} dB\n",
        "Filler Words: {filler_words}\n",
        "Pet Words: {pet_words}\n",
        "Preliminary Scores:\n",
        "- Body Language: {body_language_score}/5\n",
        "- Facial Expressions: {facial_expressions_score}/5\n",
        "- Speech Quality: {speech_quality_score}/5\n",
        "- Content Quality: {content_quality_score}/5\n",
        "Please generate the evaluation report, including justifications for each score and the final aggregated score. You can be critical with you scoring if the apt reasoning is there from your side.\"\"\")\n",
        "])\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    \"\"\"Extract audio from video using FFmpeg.\"\"\"\n",
        "    try:\n",
        "        stream = ffmpeg.input(video_path)\n",
        "        stream = ffmpeg.output(stream, audio_path, acodec='pcm_s16le', ar=16000, ac=1)\n",
        "        ffmpeg.run(stream, overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode()}\")\n",
        "        raise\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcribe audio to text using Groq's Whisper API.\"\"\"\n",
        "    try:\n",
        "        with open(audio_path, \"rb\") as file:\n",
        "            transcription = client.audio.transcriptions.create(\n",
        "                file=(os.path.basename(audio_path), file.read()),\n",
        "                model=\"whisper-large-v3\",\n",
        "                response_format=\"json\",\n",
        "                language=\"en\",\n",
        "                temperature=0.0\n",
        "            )\n",
        "            return transcription.text\n",
        "    except Exception as e:\n",
        "        print(f\"Groq transcription error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def analyze_filler_pet_words(transcript):\n",
        "    \"\"\"Analyze transcript for filler and pet words.\"\"\"\n",
        "    words = transcript.lower().split()\n",
        "    filler_words = ['and', 'that', 'really', 'now', 'just', 'um', 'uh', 'like']\n",
        "    pet_words = ['i', 'to', 'the', 'of']\n",
        "\n",
        "    filler_count = {word: words.count(word) for word in filler_words if words.count(word) > 0}\n",
        "    pet_count = {word: words.count(word) for word in pet_words if words.count(word) > 0}\n",
        "\n",
        "    total_words = len(words)\n",
        "    filler_ratio = sum(filler_count.values()) / total_words if total_words > 0 else 0\n",
        "\n",
        "    return filler_count, pet_count, filler_ratio\n",
        "\n",
        "def analyze_audio(audio_path):\n",
        "    \"\"\"Analyze audio for pitch, volume, and compute speech quality score.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path)\n",
        "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
        "        pitch_values = pitches[magnitudes > 0]\n",
        "        pitch_std = np.std(pitch_values) if len(pitch_values) > 0 else 0\n",
        "        avg_pitch = np.mean(pitch_values) if len(pitch_values) > 0 else 300\n",
        "\n",
        "        rms = librosa.feature.rms(y=y)[0]\n",
        "        volume_std = np.std(rms) if len(rms) > 0 else 0\n",
        "        avg_volume = 20 * np.log10(np.mean(rms) + 1e-10) + 60 if np.mean(rms) > 0 else 60\n",
        "\n",
        "        # Speech score\n",
        "        modulation_score = min(4.0, (pitch_std / 100 + volume_std / 0.01) * 0.8)\n",
        "        pitch_penalty = abs(avg_pitch - 300) / 1000 if avg_pitch > 0 else 0\n",
        "        volume_penalty = abs(avg_volume - 60) / 30 if avg_volume > 0 else 0\n",
        "        speech_score = min(5, max(1, modulation_score - pitch_penalty - volume_penalty))\n",
        "\n",
        "        print(f\"Pitch std: {pitch_std}, Volume std: {volume_std}, Avg pitch: {avg_pitch}, Avg volume: {avg_volume}, Speech score: {speech_score}\")\n",
        "        return pitch_std, volume_std, avg_pitch, avg_volume, speech_score\n",
        "    except Exception as e:\n",
        "        print(f\"Audio analysis error: {e}\")\n",
        "        return 0, 0, 300, 60, 3\n",
        "\n",
        "def analyze_content(transcript, filler_ratio):\n",
        "    \"\"\"Analyze transcript for content quality using Groq LLM.\"\"\"\n",
        "    content_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Evaluate the clarity, relevance, and impact of the following speech transcript for a management student presentation. Assign a score out of 5 based on:\n",
        "        - Clarity: Is the message clear and well-structured?\n",
        "        - Relevance: Is the content pertinent to management topics?\n",
        "        - Impact: Does it engage and persuade the audience?\n",
        "        Penalize for filler words (ratio: {filler_ratio:.2%}, reduce by 0.3/5 if >5%) and lengthy sentences. Target ~{content_quality_target}/5 for clear, engaging content. Provide the score only.\"\"\"),\n",
        "        (\"human\", \"{transcript}\")\n",
        "    ])\n",
        "    # Dynamic target based on filler ratio (proxy for content quality)\n",
        "    content_quality_target = 3.2 if filler_ratio > 0.05 else 3.5\n",
        "    chain = content_prompt | llm\n",
        "    response = chain.invoke({\"transcript\": transcript, \"filler_ratio\": filler_ratio, \"content_quality_target\": content_quality_target})\n",
        "    try:\n",
        "        score = float(response.content.strip())\n",
        "        # Penalize for high filler ratio\n",
        "        score = score - 0.3 if filler_ratio > 0.05 else score\n",
        "        return min(5, max(1, score))\n",
        "    except ValueError:\n",
        "        return 3.5  # Default\n",
        "\n",
        "def analyze_video(video_path, upper_torso=True):\n",
        "    \"\"\"Analyze video for body language and facial expressions using MediaPipe.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(\"Could not open video file.\")\n",
        "\n",
        "    body_language_data = []\n",
        "    expression_data = []\n",
        "    frame_count = 0\n",
        "    gesture_count = 0\n",
        "    posture_scores = []\n",
        "    hands_visible = False\n",
        "    legs_visible = False\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic.process(frame_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            left_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
        "            right_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
        "            shoulder_diff = abs(left_shoulder.y - right_shoulder.y)\n",
        "            posture_scores.append(shoulder_diff)\n",
        "\n",
        "            if results.left_hand_landmarks or results.right_hand_landmarks:\n",
        "                gesture_count += 1\n",
        "                hands_visible = True\n",
        "\n",
        "            if not upper_torso:\n",
        "                if results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_KNEE].visibility > 0.5 or \\\n",
        "                   results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_KNEE].visibility > 0.5:\n",
        "                    legs_visible = True\n",
        "\n",
        "        if results.face_landmarks:\n",
        "            mouth_left = results.face_landmarks.landmark[61]\n",
        "            mouth_right = results.face_landmarks.landmark[291]\n",
        "            mouth_dist = np.sqrt((mouth_right.x - mouth_left.x)**2 + (mouth_right.y - mouth_left.y)**2)\n",
        "            expression = \"Smiling\" if mouth_dist > 0.05 else \"Neutral\"\n",
        "            expression_data.append(expression)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    avg_posture_score = np.mean(posture_scores) if posture_scores else 0\n",
        "    posture_desc = \"Balanced\" if avg_posture_score < 0.05 else \"Unbalanced\"\n",
        "    gesture_freq = gesture_count / frame_count if frame_count > 0 else 0\n",
        "    gesture_desc = \"Frequent\" if gesture_freq > 0.1 else \"Minimal\"\n",
        "\n",
        "    # Penalties and bonuses\n",
        "    visibility_penalty = 0\n",
        "    if not hands_visible:\n",
        "        visibility_penalty += 0.5\n",
        "    if not upper_torso and not legs_visible:\n",
        "        visibility_penalty += 0.5\n",
        "    engagement_penalty = 0.5 if gesture_freq < 0.5 else 0.2\n",
        "\n",
        "    body_language = (f\"Posture: {posture_desc} (avg shoulder diff: {avg_posture_score:.3f}). \"\n",
        "                    f\"Gestures: {gesture_desc} (frequency: {gesture_freq:.3f}). \"\n",
        "                    f\"Hands visible: {hands_visible}. \"\n",
        "                    f\"{'Upper torso only evaluated.' if upper_torso else f'Legs visible: {legs_visible}.'}\")\n",
        "    body_language_score = 3.5 - (avg_posture_score / 0.1 * 0.3) + (gesture_freq * 2) - (visibility_penalty + engagement_penalty)\n",
        "    body_language_score = min(5, max(1, body_language_score))\n",
        "\n",
        "    if expression_data:\n",
        "        smile_ratio = expression_data.count(\"Smiling\") / len(expression_data)\n",
        "        expression_summary = f\"Smiling {smile_ratio*100:.1f}% of the time, Neutral otherwise.\"\n",
        "        expression_score = 2.0 if smile_ratio < 0.1 else 3.0\n",
        "    else:\n",
        "        expression_summary = \"No facial landmarks detected.\"\n",
        "        expression_score = 2.0\n",
        "\n",
        "    return body_language, expression_summary, body_language_score, expression_score\n",
        "\n",
        "def generate_report(video_path, output_path, upper_torso=True):\n",
        "    \"\"\"Generate evaluation report with scores for the video.\"\"\"\n",
        "    audio_path = \"temp_audio.wav\"\n",
        "    try:\n",
        "        extract_audio(video_path, audio_path)\n",
        "        transcript = transcribe_audio(audio_path)\n",
        "        pitch_std, volume_std, avg_pitch, avg_volume, speech_quality_score = analyze_audio(audio_path)\n",
        "        filler_count, pet_count, filler_ratio = analyze_filler_pet_words(transcript)\n",
        "        content_quality_score = analyze_content(transcript, filler_ratio)\n",
        "        body_language, facial_expressions, body_language_score, facial_expressions_score = analyze_video(video_path, upper_torso=upper_torso)\n",
        "        final_score = np.mean([body_language_score, facial_expressions_score, speech_quality_score, content_quality_score])\n",
        "\n",
        "        # Prompt parameters (dynamic based on metrics)\n",
        "        body_language_target = 2.5 if pitch_std < 800 else 3.7  # Lower for weaker modulation\n",
        "        speech_quality_target = 3.0 if pitch_std < 800 else 3.5\n",
        "        content_quality_target = 3.2 if filler_ratio > 0.05 else 3.5\n",
        "        final_score_target = 2.9 if pitch_std < 800 else 3.4\n",
        "\n",
        "        chain = prompt | llm\n",
        "        response = chain.invoke({\n",
        "            \"body_language\": body_language,\n",
        "            \"facial_expressions\": facial_expressions,\n",
        "            \"transcript\": transcript,\n",
        "            \"pitch_std\": pitch_std,\n",
        "            \"volume_std\": volume_std,\n",
        "            \"avg_pitch\": avg_pitch,\n",
        "            \"avg_volume\": avg_volume,\n",
        "            \"filler_words\": str(filler_count),\n",
        "            \"pet_words\": str(pet_count),\n",
        "            \"body_language_score\": round(body_language_score, 1),\n",
        "            \"facial_expressions_score\": round(facial_expressions_score, 1),\n",
        "            \"speech_quality_score\": round(speech_quality_score, 1),\n",
        "            \"content_quality_score\": round(content_quality_score, 1),\n",
        "            \"body_language_target\": body_language_target,\n",
        "            \"speech_quality_target\": speech_quality_target,\n",
        "            \"content_quality_target\": content_quality_target,\n",
        "            \"final_score_target\": final_score_target\n",
        "        })\n",
        "\n",
        "        report_content = f\"{response.content}\\n\\n**Final Score**: {round(final_score, 1)}/5\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            f.write(report_content)\n",
        "\n",
        "        return report_content\n",
        "    finally:\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "\n",
        "def main():\n",
        "    # Upload video file\n",
        "    print(\"Please upload your video file (e.g., speak.mp4)\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    output_path = \"evaluation_report.txt\"\n",
        "\n",
        "    # Set upper_torso flag\n",
        "    upper_torso = True\n",
        "\n",
        "    try:\n",
        "        report = generate_report(video_path, output_path, upper_torso=upper_torso)\n",
        "        print(\"Report generated successfully. Saved to\", output_path)\n",
        "        print(\"\\nReport Preview:\\n\", report[:500], \"...\")\n",
        "\n",
        "        # Download the report\n",
        "        files.download(output_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "P4qnGpO4NvAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank for using our free tool! If it is posssilbe for you then do share it along and let others benefit from the same ü§ó\n",
        "\n",
        "**Credits: Alpha AI Team (www.alphaai.biz)**"
      ],
      "metadata": {
        "id": "aK6Qkb4le27L"
      }
    }
  ]
}