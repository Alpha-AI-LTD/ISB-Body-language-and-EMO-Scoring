{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prior to starting the application, you need to obtain your free Groq API key from https://console.groq.com/keys. ⛔\n",
        "\n",
        "**Replace the key at this point in the code below ⬇️**\n",
        "\n",
        "GROQ_API_KEY = \"abcdefghijkl_1234567890\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "qQ7eeMqdb94m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY_USER = \"gsk_djt1FIWJ6TjVrSamnbkOWGdyb3FYIP0H8Qsdgfdsgfdhdfh\"  # Replace with your Groq API key"
      ],
      "metadata": {
        "id": "UUy3vraOcKK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the value for scoring criteria. Set \"True\" for whichever component you want to use and leave it \"False\" for the rest."
      ],
      "metadata": {
        "id": "SxgVok2KBanb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visibility Settings — set exactly one to True\n",
        "only_hands_visible = False\n",
        "both_hands_and_legs_visible = False\n",
        "no_hands_no_legs_visible = True"
      ],
      "metadata": {
        "id": "yQ2kST20AsCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3KozkPC7eucI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run the application Go to Runtime button in the Navigation bar above and click on the Run All option. The Code will now run automatically.\n",
        "\n",
        "# Sometimes, there could be an error that occurs when the Stage 2 code is running. Do not worry. Simply go to Runtime > Restart Session > Run All. It will start again but this time the error will not be there. It occurs because of some installation defaults in the code (No need to bother).\n",
        "\n",
        "**NOTE - Remember you have added the API key above prior to doing this step.**\n",
        "\n",
        "**For any concerns write to info@alphaai.biz**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4y-vPB2ncVtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lcd4_e6PexL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 - The dependencies will install automatically. Do not terminate the session, close the browser tab or interrrupt the execution by any means possible."
      ],
      "metadata": {
        "id": "vx0klwWUaktx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWZM1OwCNfkV"
      },
      "outputs": [],
      "source": [
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas mediapipe librosa speechrecognition opencv-python ffmpeg-python langchain-groq\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install numpy==1.26.4\n",
        "!pip install mediapipe==0.10.14\n",
        "!pip install pandas==2.2.2\n",
        "!pip install librosa==0.10.2\n",
        "!pip install speechrecognition==3.10.4\n",
        "!pip install opencv-python==4.10.0.84\n",
        "!pip install ffmpeg-python==0.2.0\n",
        "!pip install langchain-groq==0.3.0\n",
        "\n",
        "# Install Whisper for speech recognition\n",
        "!pip install openai-whisper==20231117"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If the above installation is successful then there would be a number inclosed within the square bracket. For example [1] or [2]."
      ],
      "metadata": {
        "id": "Q7O2cRala7hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yqSf_PmCez3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 - Here the code will run automatically and ask you to upload your video file for analysis."
      ],
      "metadata": {
        "id": "txhvglLDbRB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import groq\n",
        "import httpx\n",
        "print(f\"groq: {groq.__version__}, httpx: {httpx.__version__}\")"
      ],
      "metadata": {
        "id": "3gzdvxMmw9zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key=GROQ_API_KEY_USER)\n",
        "print(\"Groq client initialized!\")"
      ],
      "metadata": {
        "id": "Qs7dvmxbxAp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE - In case you do not wish to work with ranges then refer to the code below. Nothing that technical just look for the System Prompt and Report Generation and refer to the Comment/Un-Comment instructions there.**"
      ],
      "metadata": {
        "id": "PU5cu4uInfAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import files\n",
        "\n",
        "# Initialize MediaPipe Holistic\n",
        "try:\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    holistic = mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize MediaPipe: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq client\n",
        "GROQ_API_KEY = GROQ_API_KEY_USER\n",
        "try:\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq client: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize Groq LLM for report generation\n",
        "try:\n",
        "    llm = ChatGroq(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=2000,\n",
        "        api_key=GROQ_API_KEY\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Groq LLM: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# System prompt for detailed, single-score reporting\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "You are an expert in evaluating public speaking skills for management students, using a scoring system designed to replicate uSpeek's single-score output (1 to 5). Generate a detailed, student-friendly report based on the provided data. The report must include:\n",
        "\n",
        "- **Introduction**: Explain the purpose of the evaluation and the components assessed (facial expressions, speech quality, content quality).\n",
        "- **Facial Expressions**: Describe the smiling ratio and its impact on the score (2.0 for 0% smiling, up to 5.0 for 100% smiling). Highlight strengths, weaknesses, and specific improvements. Target ~{facial_expressions_target}/5.\n",
        "- **Speech Quality**: Explain modulation (pitch and volume variation), pitch (~150 Hz), and volume (~40 dB). Detail score calculation, strengths, weaknesses, and improvements. Target ~{speech_quality_target}/5.\n",
        "- **Content Quality**: Assess clarity, relevance, impact, noting filler words (penalized by 0.05 if ratio >5%). Highlight strengths, weaknesses, and improvements. Target ~{content_quality_target}/5.\n",
        "- **Final Score**: Present the single weighted average score (30% facial expressions, 30% speech quality, 40% content quality) and a +/- 0.2 range (clamped to [1,5]). Explain the weighting, score, and range in simple terms.\n",
        "- **Recommendations**: Provide specific, actionable advice for each component to help the student improve.\n",
        "\n",
        "Use a clear, encouraging, and professional tone to motivate students.\n",
        "\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "Facial Expressions: {facial_expressions}\n",
        "Speech Transcript: {transcript}\n",
        "Audio Characteristics: Pitch std: {pitch_std}, Volume std: {volume_std}, Avg pitch: {avg_pitch} Hz, Avg volume: {avg_volume} dB\n",
        "Filler Words: {filler_words}\n",
        "Pet Words: {pet_words}\n",
        "Preliminary Scores:\n",
        "- Facial Expressions: {facial_expressions_score}/5\n",
        "- Speech Quality: {speech_quality_score}/5\n",
        "- Content Quality: {content_quality_score}/5\n",
        "\"\"\"),\n",
        "])\n",
        "\n",
        "# Audio extraction\n",
        "def extract_audio(video_path, audio_path):\n",
        "    stream = ffmpeg.input(video_path)\n",
        "    stream = ffmpeg.output(stream, audio_path, acodec='pcm_s16le', ar=16000, ac=1)\n",
        "    ffmpeg.run(stream, overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
        "\n",
        "# Transcription\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        with open(audio_path, 'rb') as f:\n",
        "            res = client.audio.transcriptions.create(\n",
        "                file=(os.path.basename(audio_path), f.read()),\n",
        "                model='whisper-large-v3', response_format='json', language='en', temperature=0.0\n",
        "            )\n",
        "        return res.text\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# Filler and pet words analysis\n",
        "def analyze_filler_pet_words(transcript):\n",
        "    words = transcript.lower().split()\n",
        "    fillers = ['and','that','really','now','just','um','uh','like']\n",
        "    pets = ['i','to','the','of']\n",
        "    fcnt = {w:words.count(w) for w in fillers if words.count(w)>0}\n",
        "    pcnt = {w:words.count(w) for w in pets if words.count(w)>0}\n",
        "    total = len(words)\n",
        "    fr = sum(fcnt.values())/total if total else 0\n",
        "    return fcnt, pcnt, fr\n",
        "\n",
        "# Audio analysis for speech quality\n",
        "def analyze_audio(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    f0 = librosa.yin(y, fmin=80, fmax=500, sr=sr)\n",
        "    avg_pitch = float(np.median(f0[f0>0])) if np.any(f0>0) else 150.0\n",
        "    pitch_std = float(np.std(f0[f0>0])) if np.any(f0>0) else 0.0\n",
        "    rms = librosa.feature.rms(y=y)[0]\n",
        "    avg_volume = float(20*np.log10(np.mean(rms)+1e-10)+40)\n",
        "    volume_std = float(np.std(rms))\n",
        "    mod_score = min(4.5, (pitch_std/80 + volume_std/0.01)*0.7)\n",
        "    pitch_pen = max(0, (abs(avg_pitch - 150) - 50) / 500)\n",
        "    vol_pen = max(0, (40 - avg_volume) / 60)\n",
        "    speech_score = min(5, max(1, mod_score - pitch_pen - vol_pen))\n",
        "    return pitch_std, volume_std, avg_pitch, avg_volume, speech_score\n",
        "\n",
        "# Content quality analysis\n",
        "def analyze_content(transcript, fr):\n",
        "    p = ChatPromptTemplate.from_messages([\n",
        "        ('system', 'Evaluate clarity, relevance, impact on a scale of 1 to 5. Return only the numerical score.'),\n",
        "        ('human', '{transcript}')\n",
        "    ])\n",
        "    resp = (p | llm).invoke({'transcript': transcript})\n",
        "    try:\n",
        "        base = float(resp.content.strip())\n",
        "    except:\n",
        "        base = 4.0\n",
        "    penalty = 0.05 if fr > 0.05 else 0\n",
        "    return min(5, max(1, base - penalty))\n",
        "\n",
        "# Video analysis for facial expressions\n",
        "def analyze_video(video_path, only_hands_visible=False, both_hands_and_legs_visible=False, no_hands_no_legs_visible=False):\n",
        "    if [only_hands_visible, both_hands_and_legs_visible, no_hands_no_legs_visible].count(True) != 1:\n",
        "        raise ValueError('Set exactly one visibility flag to True')\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError('Cannot open video')\n",
        "    cnt = 0\n",
        "    expr = []\n",
        "    while True:\n",
        "        ret, frm = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        rgb = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)\n",
        "        res = holistic.process(rgb)\n",
        "        if res.face_landmarks:\n",
        "            m1 = res.face_landmarks.landmark[61]\n",
        "            m2 = res.face_landmarks.landmark[291]\n",
        "            expr.append('Smiling' if np.hypot(m2.x - m1.x, m2.y - m1.y) > 0.05 else 'Neutral')\n",
        "        cnt += 1\n",
        "    cap.release()\n",
        "    if expr:\n",
        "        sr = expr.count('Smiling') / len(expr)\n",
        "        fe_sc = min(5.0, 2.0 + sr * 3.0)\n",
        "        fe_desc = f'Smiling {sr*100:.1f}%'\n",
        "    else:\n",
        "        fe_sc = 2.0\n",
        "        fe_desc = 'No facial data'\n",
        "    return '', fe_desc, 0, fe_sc\n",
        "\n",
        "# Updated report generation with single score and verbose output\n",
        "def generate_report(video_path, output_path, only_hands_visible=False, both_hands_and_legs_visible=False, no_hands_no_legs_visible=False):\n",
        "    ap = 'temp.wav'\n",
        "    try:\n",
        "        extract_audio(video_path, ap)\n",
        "        tr = transcribe_audio(ap)\n",
        "        ps, vs, ap_pitch, av, ss = analyze_audio(ap)\n",
        "        fcnt, pcnt, fr = analyze_filler_pet_words(tr)\n",
        "        cs = analyze_content(tr, fr)\n",
        "        _, fe_desc, _, fe_sc = analyze_video(video_path, only_hands_visible, both_hands_and_legs_visible, no_hands_no_legs_visible)\n",
        "\n",
        "        # Weighted average: 30% facial, 30% speech, 40% content\n",
        "        avg_score = 0.3 * fe_sc + 0.3 * ss + 0.4 * cs\n",
        "        single_score = round(avg_score, 2)\n",
        "        low = round(max(1, avg_score - 0.2), 1)\n",
        "        high = round(min(5, avg_score + 0.2), 1)\n",
        "\n",
        "        # Targets for reporting\n",
        "        fe_target = 3.5\n",
        "        sq_target = 3.5\n",
        "        cq_target = 4.0\n",
        "        fs_target = 3.5\n",
        "\n",
        "        resp = (prompt | llm).invoke({\n",
        "            'facial_expressions': fe_desc,\n",
        "            'transcript': tr,\n",
        "            'pitch_std': ps,\n",
        "            'volume_std': vs,\n",
        "            'avg_pitch': ap_pitch,\n",
        "            'avg_volume': av,\n",
        "            'filler_words': str(fcnt),\n",
        "            'pet_words': str(pcnt),\n",
        "            'facial_expressions_score': fe_sc,\n",
        "            'speech_quality_score': ss,\n",
        "            'content_quality_score': cs,\n",
        "            'facial_expressions_target': fe_target,\n",
        "            'speech_quality_target': sq_target,\n",
        "            'content_quality_target': cq_target,\n",
        "            'final_score_target': fs_target\n",
        "        })\n",
        "\n",
        "        # Verbose, student-friendly report\n",
        "        report = f\"\"\"\n",
        "**Public Speaking Skills Evaluation Report**\n",
        "\n",
        "**Introduction**\n",
        "Welcome to your personalized public speaking evaluation! This report analyzes your performance to help you become a more confident and effective speaker. We assess three key components: **Facial Expressions**, **Speech Quality**, and **Content Quality**. Each is scored from 1 to 5, with 1 indicating significant room for improvement and 5 representing an outstanding performance. Your final score is a single value, calculated as a weighted average (30% facial expressions, 30% speech quality, 40% content quality), with a ±0.2 range to show where your performance likely falls. This approach mirrors industry-standard evaluations, like uSpeek, and provides clear, actionable feedback to guide your growth.\n",
        "\n",
        "**Facial Expressions: {fe_sc:.1f}/5**\n",
        "   - **How It’s Evaluated**: We measure your smiling ratio—the percentage of time you display a smile—using facial landmark analysis. A 0% smiling ratio scores 2.0, while 100% scores 5.0, reflecting engagement and positivity. Your smiling ratio is {fe_desc}, contributing to a score of {fe_sc:.1f}/5.\n",
        "   - **Strengths**: {'Your frequent smiling conveys warmth and connects well with the audience.' if fe_sc >= 3.5 else 'You maintain some positive expressions, setting a foundation for engagement.'}\n",
        "   - **Areas for Improvement**: {'Your smiling ratio is relatively low, which may make your delivery seem less engaging.' if fe_sc < 3.0 else 'Increasing your smiling frequency can further enhance audience connection.'}\n",
        "   - **Recommendations**: Practice speaking in front of a mirror to consciously increase your smiling, aiming for a ratio above 50%. Record your practice sessions to observe your expressions. Try engaging with your audience through eye contact and subtle nods to boost perceived positivity.\n",
        "\n",
        "**Speech Quality: {ss:.1f}/5**\n",
        "   - **How It’s Evaluated**: We analyze your voice’s modulation (variation in pitch and volume), average pitch (target 150 Hz), and average volume (target 40 dB). Pitch is evaluated within a typical voice range (100–200 Hz) without penalty, and volume penalties are minimal to reward natural delivery. Your score of {ss:.1f}/5 reflects your pitch variation (standard deviation {ps:.1f} Hz) and volume consistency ({vs:.3f}).\n",
        "   - **Strengths**: {'Your dynamic pitch and volume keep the audience engaged, showcasing strong vocal delivery.' if ss >= 3.5 else 'Your voice shows good modulation, providing a solid base for effective communication.'}\n",
        "   - **Areas for Improvement**: {'Your volume ({av:.1f} dB) is below the ideal range, potentially reducing impact.' if av < 40 else 'Further varying your pitch and volume can add more emphasis to key points.'}\n",
        "   - **Recommendations**: Practice projecting your voice to reach a volume closer to 40 dB, especially in larger settings. Use vocal exercises, like reading aloud with exaggerated intonation, to enhance pitch variation. Record and listen to your speeches to identify areas for more dynamic delivery.\n",
        "\n",
        "**Content Quality: {cs:.1f}/5**\n",
        "   - **How It’s Evaluated**: Your speech’s clarity, relevance, and impact are scored from 1 to 5, with a base score reflecting content strength. Filler words ({fcnt}) and pet words ({pcnt}) are analyzed, with a 0.05 penalty if fillers exceed 5% of your speech. Your score of {cs:.1f}/5 indicates a {'strong' if cs >= 4.0 else 'solid'} message with {'minor' if fr <= 0.05 else 'some'} filler word usage.\n",
        "   - **Strengths**: {'Your content is clear, relevant, and impactful, effectively conveying your message.' if cs >= 4.0 else 'Your speech is well-structured, making your points accessible to the audience.'}\n",
        "   - **Areas for Improvement**: {'Excessive filler words slightly disrupt fluency.' if fr > 0.05 else 'Reducing minor filler words and adding vivid examples can elevate impact.'}\n",
        "   - **Recommendations**: Practice pausing instead of using fillers (e.g., “um,” “like”) to gather thoughts. Rehearse your speech to build fluency and reduce repetitive words. Incorporate stories, data, or examples to make your content more engaging and memorable.\n",
        "\n",
        "**Final Score: {single_score:.2f}/5**\n",
        "   - **Calculation**: Your final score is a single value, calculated as a weighted average to reflect the importance of each component:\n",
        "     \\[\n",
        "     (0.3 \\times {fe_sc:.1f}) + (0.3 \\times {ss:.1f}) + (0.4 \\times {cs:.1f}) = {single_score:.2f}\n",
        "     \\]\n",
        "   - **Score Range**: To provide context, we include a ±0.2 range: **{low:.1f}–{high:.1f}/5**. This shows the likely range of your performance, with 1 being “needs significant improvement” and 5 being “outstanding.” Your score of {single_score:.2f}/5 indicates {'a strong performance with room to shine' if single_score >= 3.5 else 'a solid foundation with clear areas for growth'}.\n",
        "   - **Interpretation**: Your score aligns with industry-standard evaluations, like uSpeek, and reflects your ability to engage and communicate effectively. {'You’re on track to become an excellent speaker with targeted improvements.' if single_score >= 3.5 else 'With practice, you can boost your score and impact.'}\n",
        "\n",
        "**Recommendations for Improvement**\n",
        "   - **Facial Expressions**: Increase your smiling frequency to convey enthusiasm. Practice in front of a mirror or record yourself to monitor progress. Aim for a smiling ratio above 50% to enhance audience connection.\n",
        "   - **Speech Quality**: Project your voice louder (closer to 40 dB) to ensure clarity and impact. Practice varying pitch during key points using vocal exercises, like reading with exaggerated intonation. Review recordings to refine dynamism.\n",
        "   - **Content Quality**: Reduce filler words by pausing deliberately. Rehearse multiple times to improve fluency. Add compelling stories or examples to make your content more memorable and impactful.\n",
        "   - **General Tips**: Join a public speaking group, like Toastmasters, for regular practice and feedback. Record and analyze your speeches to track improvement. Seek peer or instructor feedback to identify subtle areas for growth.\n",
        "\n",
        "**Conclusion**\n",
        "Your public speaking performance demonstrates {'notable strengths' if single_score >= 3.5 else 'a promising foundation'}, particularly in {'speech quality and content' if ss >= 3.5 and cs >= 3.5 else 'specific areas'}. By focusing on enhancing facial expressions, refining vocal delivery, and polishing content, you can elevate your skills and captivate your audience. Keep practicing, use this feedback to guide your efforts, and you’ll continue to grow as a confident communicator!\n",
        "\n",
        "**Final Score: {single_score:.2f}/5 (Range: {low:.1f}–{high:.1f}/5)**\n",
        "\"\"\"\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(report)\n",
        "        return report\n",
        "    finally:\n",
        "        if os.path.exists(ap):\n",
        "            os.remove(ap)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print('Upload your video')\n",
        "    u = files.upload()\n",
        "    if not u:\n",
        "        print('No file')\n",
        "        return\n",
        "    vp = list(u.keys())[0]\n",
        "    op = 'evaluation_report.txt'\n",
        "    rep = generate_report(vp, op, only_hands_visible=False, both_hands_and_legs_visible=False, no_hands_no_legs_visible=True)\n",
        "    print(rep)\n",
        "    files.download(op)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "P4qnGpO4NvAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank for using our free tool! If it is posssilbe for you then do share it along and let others benefit from the same 🤗\n",
        "\n",
        "**Credits: Alpha AI Team (www.alphaai.biz)**"
      ],
      "metadata": {
        "id": "aK6Qkb4le27L"
      }
    }
  ]
}